{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cPickle as pkl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import csv\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, linewidth=130)\n",
    "import os\n",
    "\n",
    "os.environ[\"THEANO_FLAGS\"] = (\"device=cpu,floatX=float32\")\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "\n",
    "import variables\n",
    "\n",
    "from custom_layers import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embeddings(embeddingfn, dictfn):\n",
    "    return np.load(embeddingfn), np.load(dictfn)\n",
    "\n",
    "embeddingfn = 'cache/embedding_weights.npy'\n",
    "dictfn = 'cache/ids_dictionary.pkl'\n",
    "embedding_weights, ids_dict = read_embeddings(embeddingfn, dictfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(xtrainfn, ytrainfn, ztrainfn, qtrainfn):\n",
    "    val_split = variables.VALIDATION_SPLIT\n",
    "    \n",
    "    X_all = np.load(xtrainfn)\n",
    "    y_all = np.load(ytrainfn)\n",
    "    title_all = np.load(ttrainfn)\n",
    "    rating_all = np.load(rtrainfn)\n",
    "    \n",
    "    seed = 821  # np.random.randint(1234)\n",
    "    print 'Seed {}'.format(seed)\n",
    "    \n",
    "    nb_training_samples = int((1.-val_split)*X_all.shape[0])\n",
    "    np.random.seed(seed)\n",
    "    train_set = np.random.choice(np.arange(X_all.shape[0]),size=nb_training_samples, replace=False)\n",
    "    val_set = np.delete(np.arange(X_all.shape[0]),train_set)\n",
    "       \n",
    "    X_train = X_all[train_set,:]      \n",
    "    y_train = y_all[train_set,:]\n",
    "    title_train = title_all[train_set,:]\n",
    "    rating_train = rating_all[train_set,:]\n",
    "\n",
    "    X_val = X_all[val_set,:]    \n",
    "    y_val = y_all[val_set,:]\n",
    "    title_val = title_all[val_set,:]\n",
    "    rating_val = rating_all[val_set,:]\n",
    "\n",
    "    return X_train, y_train, title_train, rating_train, X_val, y_val, title_val, rating_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 821\n",
      "Train set:(64000, 90),(64000, 1),(64000, 90),(64000, 1)\n",
      "Val set:(16000, 90),(16000, 1),(16000, 90),(16000, 1)\n",
      "Test set:(36395, 90),(36395, 90),(36395, 1)\n"
     ]
    }
   ],
   "source": [
    "xtrainfn = 'cache/train/data.npy'\n",
    "ytrainfn = 'cache/train/labels.npy'\n",
    "ttrainfn = 'cache/train/title.npy'\n",
    "rtrainfn = 'cache/train/rating.npy'\n",
    "\n",
    "(X_train, y_train, title_train, rating_train, \n",
    "    X_val, y_val, title_val, rating_val) = read_data(xtrainfn,ytrainfn, ttrainfn, rtrainfn)\n",
    "print \"Train set:{},{},{},{}\\nVal set:{},{},{},{}\".format(X_train.shape, y_train.shape, \n",
    "                                                          title_train.shape, rating_train.shape,\n",
    "                                                          X_val.shape, y_val.shape, \n",
    "                                                          title_val.shape, rating_val.shape)\n",
    "\n",
    "X_test = np.load('cache/test/data.npy')\n",
    "title_test = np.load('cache/test/title.npy')\n",
    "rating_test = np.load('cache/test/rating.npy')\n",
    "print \"Test set:{},{},{}\".format(X_test.shape, title_test.shape, rating_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_lstm(embedding_weights, verbose=True):\n",
    "    lstm_output = 64\n",
    "    lstm_dropout_w = 0.3\n",
    "    lstm_dropout_u = 0.3\n",
    "    dropout = 0.6\n",
    "    maxlen = variables.MAX_LEN\n",
    "\n",
    "    nb_words, embedding_dim = embedding_weights.shape\n",
    "    \n",
    "    embedding = Embedding(input_dim=nb_words,             \n",
    "                          input_length=maxlen,            \n",
    "                          output_dim=embedding_dim,       \n",
    "                          mask_zero=True,                 \n",
    "                          weights=[embedding_weights],    \n",
    "                          trainable=False,\n",
    "                          name='embedding')\n",
    "    lstm = LSTM(lstm_output,                         \n",
    "                dropout=lstm_dropout_u,            \n",
    "                recurrent_dropout=lstm_dropout_w,\n",
    "                return_sequences=False,\n",
    "                name='lstm')\n",
    "\n",
    "    # Inputs\n",
    "    data = Input(shape=(maxlen,), name='data')\n",
    "    title =Input(shape=(maxlen,), name='title')\n",
    "    rating = Input(shape=(1,), name='rating')\n",
    "    # Embeddings\n",
    "    data_emb = embedding(data)\n",
    "    title_emb = embedding(title)\n",
    "    # LSTM\n",
    "    data_m = lstm(data_emb)\n",
    "    title_m = lstm(title_emb)\n",
    "    # Merge\n",
    "    m = concatenate([data_m, title_m, rating])\n",
    "    #Output\n",
    "    s = Dropout(dropout)(m)\n",
    "    s = Dense(1, name='dense')(s)\n",
    "    s = Activation('sigmoid')(s)\n",
    "    model = Model(inputs=[data,title,rating],outputs=[s])\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    def schedule(epoch):\n",
    "        if epoch >= 5:\n",
    "            return 0.005\n",
    "        else:\n",
    "            return 0.005\n",
    "            \n",
    "    return model, schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_bilstm(embedding_weights, verbose=True):\n",
    "    lstm_output = 32\n",
    "    lstm_dropout_w = 0.3\n",
    "    lstm_dropout_u = 0.3\n",
    "    dropout = 0.6\n",
    "    maxlen = variables.MAX_LEN\n",
    "\n",
    "    nb_words, embedding_dim = embedding_weights.shape\n",
    "    \n",
    "    embedding = Embedding(input_dim=nb_words,             \n",
    "                          input_length=maxlen,            \n",
    "                          output_dim=embedding_dim,       \n",
    "                          mask_zero=True,                 \n",
    "                          weights=[embedding_weights],    \n",
    "                          trainable=False,\n",
    "                          name='embedding')\n",
    "    lstm = Bidirectional(LSTM(lstm_output,                         \n",
    "                dropout=lstm_dropout_u,            \n",
    "                recurrent_dropout=lstm_dropout_w,\n",
    "                return_sequences=False,\n",
    "                name='lstm'))\n",
    "\n",
    "    # Inputs\n",
    "    data = Input(shape=(maxlen,), name='data')\n",
    "    title =Input(shape=(maxlen,), name='title')\n",
    "    rating = Input(shape=(1,), name='rating')\n",
    "    # Embeddings\n",
    "    data_emb = embedding(data)\n",
    "    title_emb = embedding(title)\n",
    "    # LSTM\n",
    "    data_m = lstm(data_emb)\n",
    "    title_m = lstm(title_emb)\n",
    "    # Merge\n",
    "    m = concatenate([data_m, title_m, rating])\n",
    "    #Output\n",
    "    s = Dropout(dropout)(m)\n",
    "    s = Dense(1, name='dense')(s)\n",
    "    s = Activation('sigmoid')(s)\n",
    "    model = Model(inputs=[data,title,rating],outputs=[s])\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    def schedule(epoch):\n",
    "        if epoch >= 5:\n",
    "            return 0.005\n",
    "        else:\n",
    "            return 0.005\n",
    "            \n",
    "    return model, schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_lstm(lstm_output, embedding_weights, verbose=True):\n",
    "    lstm_dropout_w = 0.3\n",
    "    lstm_dropout_u = 0.3\n",
    "    dropout = 0.6\n",
    "    maxlen = variables.MAX_LEN\n",
    "\n",
    "    nb_words, embedding_dim = embedding_weights.shape\n",
    "    \n",
    "    embedding = Embedding(input_dim=nb_words,             \n",
    "                          input_length=maxlen,            \n",
    "                          output_dim=embedding_dim,       \n",
    "                          mask_zero=True,                 \n",
    "                          weights=[embedding_weights],    \n",
    "                          trainable=False,\n",
    "                          name='embedding')\n",
    "    lstm = LSTM(lstm_output,                         \n",
    "                dropout=lstm_dropout_u,            \n",
    "                recurrent_dropout=lstm_dropout_w,\n",
    "                return_sequences=True,\n",
    "                name='lstm')\n",
    "    attention = Attention()\n",
    "\n",
    "    # Inputs\n",
    "    data = Input(shape=(maxlen,), name='data')\n",
    "    title =Input(shape=(maxlen,), name='title')\n",
    "    rating = Input(shape=(1,), name='rating')\n",
    "    # Embeddings\n",
    "    data_emb = embedding(data)\n",
    "    title_emb = embedding(title)\n",
    "    # LSTM\n",
    "    data_m = lstm(data_emb)\n",
    "    data_m = attention(data_m)\n",
    "    \n",
    "    title_m = lstm(title_emb)\n",
    "    title_m = attention(title_m)\n",
    "    # Merge\n",
    "    m = concatenate([data_m, title_m, rating])\n",
    "    #Output\n",
    "    s = Dropout(dropout)(m)\n",
    "    s = Dense(1, name='dense')(s)\n",
    "    s = Activation('sigmoid')(s)\n",
    "    model = Model(inputs=[data,title,rating],outputs=[s])\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    def schedule(epoch):\n",
    "        if epoch >= 5:\n",
    "            return 0.005\n",
    "        else:\n",
    "            return 0.005\n",
    "            \n",
    "    return model, schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AUC(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "        self.X_val, self.title_val, self.rating_val, self.y_val = X_val, title_val, rating_val, y_val\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict([self.X_val, self.title_val, self.rating_val], verbose=0)\n",
    "        score = roc_auc_score(self.y_val, y_pred)\n",
    "        print ' - val_auc - {:.4f}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,z_train,q_train,X_val,y_val,z_val,q_val,embedding_weights):\n",
    "    nb_epochs = 100\n",
    "    batch_size = 128\n",
    "    patience = 3\n",
    "    is_earlyStopping = True\n",
    "\n",
    "    with_val = X_val.shape[0] > 0\n",
    "    \n",
    "    # build neural network\n",
    "    model, schedule = simple_lstm(embedding_weights)\n",
    "    #model, schedule = simple_bilstm(embedding_weights)\n",
    "    #model, schedule = attention_lstm(64, embedding_weights)\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    # AUC callback\n",
    "    auc = AUC()\n",
    "    \n",
    "    # learning rate scheduler\n",
    "    lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "    # stop training if the testing loss stops decreasing\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "    \n",
    "    callbacks = [lr_scheduler, auc]\n",
    "    if is_earlyStopping and with_val:\n",
    "        callbacks += [early_stopping]\n",
    "        \n",
    "    # Train model\n",
    "    try:\n",
    "        model.fit([X_train,title_train,rating_train], y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=nb_epochs,\n",
    "                  validation_data=([X_val,title_val,rating_val], y_val),\n",
    "                  callbacks=callbacks,\n",
    "                  verbose=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print '\\nTraining interrupted by user.'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "data (InputLayer)                (None, 90)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "title (InputLayer)               (None, 90)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 90, 200)       31112800                                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm (LSTM)                      (None, 64)            67840                                        \n",
      "____________________________________________________________________________________________________\n",
      "rating (InputLayer)              (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 129)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 129)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 1)             130                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1)             0                                            \n",
      "====================================================================================================\n",
      "Total params: 31,180,770\n",
      "Trainable params: 67,970\n",
      "Non-trainable params: 31,112,800\n",
      "____________________________________________________________________________________________________\n",
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/100\n",
      "63872/64000 [============================>.] - ETA: 0s - loss: 0.6515 - val_auc - 0.6911\n",
      "64000/64000 [==============================] - 285s - loss: 0.6514 - val_loss: 0.6310\n",
      "Epoch 2/100\n",
      "63872/64000 [============================>.] - ETA: 0s - loss: 0.6299 - val_auc - 0.6975\n",
      "64000/64000 [==============================] - 295s - loss: 0.6299 - val_loss: 0.6164\n",
      "Epoch 3/100\n",
      "63872/64000 [============================>.] - ETA: 0s - loss: 0.6214 - val_auc - 0.7021\n",
      "64000/64000 [==============================] - 295s - loss: 0.6215 - val_loss: 0.6131\n",
      "Epoch 4/100\n",
      "63872/64000 [============================>.] - ETA: 0s - loss: 0.6175 - val_auc - 0.7067\n",
      "64000/64000 [==============================] - 295s - loss: 0.6174 - val_loss: 0.6088\n",
      "Epoch 5/100\n",
      "63872/64000 [============================>.] - ETA: 0s - loss: 0.6123 - val_auc - 0.7075\n",
      "64000/64000 [==============================] - 300s - loss: 0.6123 - val_loss: 0.6073\n",
      "Epoch 6/100\n",
      "63872/64000 [============================>.] - ETA: 0s - loss: 0.6108 - val_auc - 0.7084\n",
      "64000/64000 [==============================] - 301s - loss: 0.6107 - val_loss: 0.6077\n",
      "Epoch 7/100\n",
      "63872/64000 [============================>.] - ETA: 0s - loss: 0.6113 - val_auc - 0.7071\n",
      "64000/64000 [==============================] - 311s - loss: 0.6113 - val_loss: 0.6068\n",
      "Epoch 8/100\n",
      " 2560/64000 [>.............................] - ETA: 261s - loss: 0.6043\n",
      "Training interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "model = train(X_train,y_train,title_train,rating_train,X_val,y_val,title_val,rating_val,embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 27s    \n"
     ]
    }
   ],
   "source": [
    "# Use in weighted lstm case\n",
    "\n",
    "#model,_ = simple_lstm(embedding_weights)\n",
    "#model,_ = attention_lstm(embedding_weights)\n",
    "#model.load_weights('models/simple_lstm64.h5')\n",
    "\n",
    "pred = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.705733232823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_auc = roc_auc_score(y_val, pred)\n",
    "print \"\\nValidation AUC: {}\\n\".format(val_auc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_pred = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "with open('cache/simple_lstm64_output.csv', 'wb') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "    csvwriter.writerow(['ID', 'Target'])\n",
    "    for i in range(test_pred.size):\n",
    "        csvwriter.writerow([i+80000, test_pred[i,0]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model.save_weights('models/simple_lstm64.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of already trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 26s    \n",
      "16000/16000 [==============================] - 28s    \n",
      "16000/16000 [==============================] - 17s    \n",
      "16000/16000 [==============================] - 27s    \n"
     ]
    }
   ],
   "source": [
    "model,_ = simple_lstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_lstm64.h5')\n",
    "pred1 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = simple_bilstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_bilstm32.h5')\n",
    "pred2 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_lstm(32, embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_lstm32.h5')\n",
    "pred3 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_lstm(64, embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_lstm64.h5')\n",
    "pred4 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.7057 - 0.7095 - 0.7097 - 0.7123 - 0.7217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble = (1*pred1 + \n",
    "            1*pred2 + \n",
    "            1*pred3 + \n",
    "            1*pred4)/4\n",
    "\n",
    "val_auc_1 = roc_auc_score(y_val, pred1)\n",
    "val_auc_2 = roc_auc_score(y_val, pred2)\n",
    "val_auc_3 = roc_auc_score(y_val, pred3)\n",
    "val_auc_4 = roc_auc_score(y_val, pred4)\n",
    "val_auc_ensemble = roc_auc_score(y_val, ensemble)\n",
    "print \"\\nValidation AUC: {:.4f} - {:.4f} - {:.4f} - {:.4f} - {:.4f}\\n\".format(val_auc_1, val_auc_2, \n",
    "                                                                              val_auc_3, val_auc_4, \n",
    "                                                                              val_auc_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36395/36395 [==============================] - 62s    \n",
      "36395/36395 [==============================] - 70s    \n",
      "36395/36395 [==============================] - 43s    \n",
      "36395/36395 [==============================] - 62s    \n"
     ]
    }
   ],
   "source": [
    "model,_ = simple_lstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_lstm64.h5')\n",
    "test_pred1 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = simple_bilstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_bilstm32.h5')\n",
    "test_pred2 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_lstm(32, embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_lstm32.h5')\n",
    "test_pred3 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_lstm(64, embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_lstm64.h5')\n",
    "test_pred4 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "test_ensemble = (test_pred1 + test_pred2 + test_pred3 + test_pred4)/4\n",
    "\n",
    "with open('cache/ensemble_output.csv', 'wb') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "    csvwriter.writerow(['ID', 'Target'])\n",
    "    for i in range(test_ensemble.size):\n",
    "        csvwriter.writerow([i+80000, test_ensemble[i,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
