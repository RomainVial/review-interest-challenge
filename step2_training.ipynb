{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import variables\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, linewidth=130)\n",
    "\n",
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = (\"device=cpu,floatX=float32\")\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from models import cbow, lstm, bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embeddings(embeddingfn):\n",
    "    return np.load(embeddingfn)\n",
    "\n",
    "embeddingfn = 'cache/embedding_weights.npy'\n",
    "embedding_weights = read_embeddings(embeddingfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train':{}, 'val':{}, 'test':{}}\n",
    "\n",
    "# Loading data from the training set\n",
    "X_text_vec = np.load('cache/train/text_vec.npy')\n",
    "X_text_ids = np.load('cache/train/text_ids.npy')\n",
    "X_titl_vec = np.load('cache/train/titl_vec.npy')\n",
    "X_titl_ids = np.load('cache/train/titl_ids.npy')\n",
    "X_ratg = np.load('cache/train/ratg.npy')\n",
    "y = np.load('cache/train/labels.npy')\n",
    "\n",
    "# Creating train and val splits\n",
    "val_split = variables.VALIDATION_SPLIT\n",
    "size = X_text_vec.shape[0]\n",
    "\n",
    "seed = 821  # np.random.randint(1234)\n",
    "print 'Seed {}'.format(seed)\n",
    "\n",
    "nb_training_samples = int((1.-val_split)*size)\n",
    "np.random.seed(seed)\n",
    "train_set = np.random.choice(np.arange(size),size=nb_training_samples, replace=False)\n",
    "val_set = np.delete(np.arange(size),train_set)\n",
    "\n",
    "# Adding splits to the dictionary\n",
    "data['train']['text_vec'] = X_text_vec[train_set,:]\n",
    "data['train']['text_ids'] = X_text_ids[train_set,:]\n",
    "data['train']['titl_vec'] = X_titl_vec[train_set,:]\n",
    "data['train']['titl_ids'] = X_titl_ids[train_set,:]\n",
    "data['train']['ratg'] = X_ratg[train_set,:]\n",
    "data['train']['y'] = y[train_set,:]\n",
    "\n",
    "data['val']['text_vec'] = X_text_vec[val_set,:]\n",
    "data['val']['text_ids'] = X_text_ids[val_set,:]\n",
    "data['val']['titl_vec'] = X_titl_vec[val_set,:]\n",
    "data['val']['titl_ids'] = X_titl_ids[val_set,:]\n",
    "data['val']['ratg'] = X_ratg[val_set,:]\n",
    "data['val']['y'] = y[val_set,:]\n",
    "\n",
    "data['test']['text_vec'] = np.load('cache/test/text_vec.npy')\n",
    "data['test']['text_ids'] = np.load('cache/test/text_ids.npy')\n",
    "data['test']['titl_vec'] = np.load('cache/test/titl_vec.npy')\n",
    "data['test']['titl_ids'] = np.load('cache/test/titl_ids.npy')\n",
    "data['test']['ratg'] = np.load('cache/test/ratg.npy')\n",
    "\n",
    "print 'Train:', data['train']['text_vec'].shape\n",
    "print 'Val:', data['val']['text_vec'].shape\n",
    "print 'Val:', data['test']['text_vec'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AUC(Callback):\n",
    "    def __init__(self, data_val):\n",
    "        super(Callback, self).__init__()\n",
    "        self.data = data_val[0]\n",
    "        self.labl = data_val[1]\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.data, verbose=0)\n",
    "        score = roc_auc_score(self.labl, y_pred)\n",
    "        print ' - val_auc - {:.4f}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, data_train, data_val):\n",
    "    nb_epochs = 100\n",
    "    batch_size = 128\n",
    "    is_earlyStopping = True\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipnorm=1.)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    # Callbacks\n",
    "    auc = AUC(data_val)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='auto', epsilon=0.001)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    \n",
    "    callbacks = [reduce_lr, auc]\n",
    "    if is_earlyStopping:\n",
    "        callbacks += [early_stopping]\n",
    "        \n",
    "    # Train model\n",
    "    try:\n",
    "        model.fit(data_train[0], data_train[1],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=nb_epochs,\n",
    "                  validation_data=(data_val[0], data_val[1]),\n",
    "                  callbacks=callbacks,\n",
    "                  verbose=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print '\\nTraining interrupted by user.'\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model, data_train, data_val, data_test = cbow(data, embedding_weights, verbose=True)\n",
    "#model, data_train, data_val, data_test = lstm(data, embedding_weights, verbose=True)\n",
    "model, data_train, data_val, data_test = bilstm(data, embedding_weights, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(model, data_train, data_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.load_weights('models/lstm_CTR_64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(data_val[0], batch_size=128, verbose=True)\n",
    "val_auc = roc_auc_score(data_val[1], pred)\n",
    "print \"\\nValidation AUC: {}\".format(val_auc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model.save_weights('models/lstm_CTR_64.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(data_test, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output/output.csv', 'wb') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "    csvwriter.writerow(['ID', 'Target'])\n",
    "    for i in range(test_pred.size):\n",
    "        if np.amax(data_test[0][i,:]) == 0:\n",
    "            # If there are no embeddings for the review, it probably\n",
    "            # means that the orignial message is not informative\n",
    "            csvwriter.writerow([i+80000, 0.])\n",
    "        else:\n",
    "            csvwriter.writerow([i+80000, test_pred[i,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (TODO) Ensemble of already trained models "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model,_ = simple_lstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_lstm64.h5')\n",
    "pred1 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = simple_bilstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_bilstm32.h5')\n",
    "pred2 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_lstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_lstm64.h5')\n",
    "pred3 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_bilstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_bilstm32.h5')\n",
    "pred4 = model.predict([X_val,title_val,rating_val], batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def find_best_weights():\n",
    "    samples = 1000\n",
    "    grid = np.random.uniform(size=(samples,4))\n",
    "    grid = grid / np.sum(grid, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    results = np.zeros((samples,))\n",
    "    \n",
    "    for i in range(samples):\n",
    "        ensemble = (grid[i,0]*pred1 + grid[i,1]*pred2 + grid[i,2]*pred3 + grid[i,3]*pred4)\n",
    "        results[i] = roc_auc_score(y_val, ensemble)\n",
    "    print 'Best ensemble AUC: {:.6f} with weights'.format(np.amax(results)), grid[np.argmax(results),:]\n",
    "    return grid[np.argmax(results),:]\n",
    "\n",
    "\n",
    "ensemble = (0.25*pred1 + 0.25*pred2 + 0.25*pred3 + 0.25*pred4)\n",
    "val_auc_1 = roc_auc_score(y_val, pred1)\n",
    "val_auc_2 = roc_auc_score(y_val, pred2)\n",
    "val_auc_3 = roc_auc_score(y_val, pred3)\n",
    "val_auc_4 = roc_auc_score(y_val, pred4)\n",
    "val_auc_ensemble = roc_auc_score(y_val, ensemble)\n",
    "print (\"\\nValidation AUC: {:.4f} - {:.4f} - {:.4f} - {:.4f}\\n\"+\n",
    "       \"Ensemble {:.4f}\\n\").format(val_auc_1, val_auc_2, val_auc_3, val_auc_4, val_auc_ensemble)\n",
    "\n",
    "weights = find_best_weights()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model,_ = simple_lstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_lstm64.h5')\n",
    "test_pred1 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = simple_bilstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/simple_bilstm32.h5')\n",
    "test_pred2 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_lstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_lstm64.h5')\n",
    "test_pred3 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "model,_ = attention_bilstm(embedding_weights, verbose=False)\n",
    "model.load_weights('models/attention_bilstm32.h5')\n",
    "test_pred4 = model.predict([X_test,title_test,rating_test], batch_size=128, verbose=True)\n",
    "\n",
    "test_ensemble = (weights[0]*test_pred1 + weights[1]*test_pred2 + weights[2]*test_pred3 + weights[3]*test_pred4)\n",
    "\n",
    "with open('output/ensemble_output.csv', 'wb') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "    csvwriter.writerow(['ID', 'Target'])\n",
    "    for i in range(test_ensemble.size):\n",
    "        csvwriter.writerow([i+80000, test_ensemble[i,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
