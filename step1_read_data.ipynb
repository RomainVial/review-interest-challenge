{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'fr'\u001b[0m\n",
      "\n",
      "    Only loading the 'fr' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import variables\n",
    "import cPickle as pkl\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    dictfn = 'data/word2vec_fr_200'\n",
    "    \n",
    "    vocab_size = 0\n",
    "    with open(dictfn, 'r') as dictfile:\n",
    "        for _ in dictfile:\n",
    "            vocab_size += 1\n",
    "    \n",
    "    nb_words = vocab_size + 1\n",
    "    embedding_dim = variables.EMBEDDING_DIM\n",
    "    embedding_weights = np.zeros((nb_words, embedding_dim))\n",
    "    ids_dict = {}\n",
    "    \n",
    "    with open(dictfn) as w2v:\n",
    "        word_id = 1\n",
    "        for line in w2v:\n",
    "            words = line.split()\n",
    "            embedding_weights[word_id, :] = map(float, words[1:])\n",
    "            ids_dict[words[0]] = word_id\n",
    "            word_id += 1\n",
    "    \n",
    "        with open('cache/ids_dictionary.pkl', 'wb') as idsfile:\n",
    "            pkl.dump(ids_dict, idsfile)\n",
    "        \n",
    "        np.save('cache/embedding_weights.npy', embedding_weights)\n",
    "load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens2vec(tokens, ids_dict):\n",
    "    ids_vec = np.zeros((1, variables.MAX_LEN))\n",
    "    \n",
    "    for idx,token in enumerate(tokens):\n",
    "        if idx >= variables.MAX_LEN:\n",
    "            break\n",
    "        try:\n",
    "            ids_vec[0,idx] = ids_dict[token.text.lower()]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return ids_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('data/output_train.csv','rU') as csvfile:\n",
    "    filereader = csv.reader(csvfile, delimiter=';')\n",
    "    y_train = []\n",
    "    for idx,row in enumerate(filereader):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        y_train.append(int(row[1]))\n",
    "    y_train = np.asarray(y_train)[:,np.newaxis]\n",
    "    np.save('cache/train/labels.npy', y_train)\n",
    "    \n",
    "    print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 1000\n",
      "Processing index 2000\n",
      "Processing index 3000\n",
      "Processing index 4000\n",
      "Processing index 5000\n",
      "Processing index 6000\n",
      "Processing index 7000\n",
      "Processing index 8000\n",
      "Processing index 9000\n",
      "Processing index 10000\n",
      "Processing index 11000\n",
      "Processing index 12000\n",
      "Processing index 13000\n",
      "Processing index 14000\n",
      "Processing index 15000\n",
      "Processing index 16000\n",
      "Processing index 17000\n",
      "Processing index 18000\n",
      "Processing index 19000\n",
      "Processing index 20000\n",
      "Processing index 21000\n",
      "Processing index 22000\n",
      "Processing index 23000\n",
      "Processing index 24000\n",
      "Processing index 25000\n",
      "Processing index 26000\n",
      "Processing index 27000\n",
      "Processing index 28000\n",
      "Processing index 29000\n",
      "Processing index 30000\n",
      "Processing index 31000\n",
      "Processing index 32000\n",
      "Processing index 33000\n",
      "Processing index 34000\n",
      "Processing index 35000\n",
      "Processing index 36000\n",
      "Processing index 37000\n",
      "Processing index 38000\n",
      "Processing index 39000\n",
      "Processing index 40000\n",
      "Processing index 41000\n",
      "Processing index 42000\n",
      "Processing index 43000\n",
      "Processing index 44000\n",
      "Processing index 45000\n",
      "Processing index 46000\n",
      "Processing index 47000\n",
      "Processing index 48000\n",
      "Processing index 49000\n",
      "Processing index 50000\n",
      "Processing index 51000\n",
      "Processing index 52000\n",
      "Processing index 53000\n",
      "Processing index 54000\n",
      "Processing index 55000\n",
      "Processing index 56000\n",
      "Processing index 57000\n",
      "Processing index 58000\n",
      "Processing index 59000\n",
      "Processing index 60000\n",
      "Processing index 61000\n",
      "Processing index 62000\n",
      "Processing index 63000\n",
      "Processing index 64000\n",
      "Processing index 65000\n",
      "Processing index 66000\n",
      "Processing index 67000\n",
      "Processing index 68000\n",
      "Processing index 69000\n",
      "Processing index 70000\n",
      "Processing index 71000\n",
      "Processing index 72000\n",
      "Processing index 73000\n",
      "Processing index 74000\n",
      "Processing index 75000\n",
      "Processing index 76000\n",
      "Processing index 77000\n",
      "Processing index 78000\n",
      "Processing index 79000\n",
      "Processing index 80000\n",
      "(80000, 90)\n",
      "(80000, 90)\n",
      "(80000,)\n"
     ]
    }
   ],
   "source": [
    "with open('data/input_train.csv','rU') as csvfile:\n",
    "    dictfn = 'cache/ids_dictionary.pkl'\n",
    "    with open(dictfn) as dictfile:\n",
    "        ids_dict = pkl.load(dictfile)\n",
    "        \n",
    "    filereader = csv.reader(csvfile, delimiter=';')\n",
    "    \n",
    "    X_train = np.zeros((0, variables.MAX_LEN))\n",
    "    title_train = np.zeros((0, variables.MAX_LEN))\n",
    "    rating_train = []\n",
    "\n",
    "    for idx,row in enumerate(filereader):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        if (idx+1) % 1000 == 0 :\n",
    "            print 'Processing index {}'.format(idx+1)\n",
    "        \n",
    "        text = nlp(unicode(row[1], 'utf-8'))\n",
    "        X_train = np.concatenate((X_train, tokens2vec(text, ids_dict)))\n",
    "        \n",
    "        title = nlp(unicode(row[2], 'utf-8'))\n",
    "        title_train = np.concatenate((title_train, tokens2vec(title, ids_dict)))\n",
    "        \n",
    "        rating_train.append(int(row[3]))\n",
    "    \n",
    "    np.save('cache/train/data.npy', X_train)\n",
    "    np.save('cache/train/title.npy', title_train)\n",
    "    np.save('cache/train/rating.npy', np.asarray(rating_train)[:,np.newaxis])\n",
    "       \n",
    "    print X_train.shape\n",
    "    print title_train.shape\n",
    "    print np.asarray(rating_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 1000\n",
      "Processing index 2000\n",
      "Processing index 3000\n",
      "Processing index 4000\n",
      "Processing index 5000\n",
      "Processing index 6000\n",
      "Processing index 7000\n",
      "Processing index 8000\n",
      "Processing index 9000\n",
      "Processing index 10000\n",
      "Processing index 11000\n",
      "Processing index 12000\n",
      "Processing index 13000\n",
      "Processing index 14000\n",
      "Processing index 15000\n",
      "Processing index 16000\n",
      "Processing index 17000\n",
      "Processing index 18000\n",
      "Processing index 19000\n",
      "Processing index 20000\n",
      "Processing index 21000\n",
      "Processing index 22000\n",
      "Processing index 23000\n",
      "Processing index 24000\n",
      "Processing index 25000\n",
      "Processing index 26000\n",
      "Processing index 27000\n",
      "Processing index 28000\n",
      "Processing index 29000\n",
      "Processing index 30000\n",
      "Processing index 31000\n",
      "Processing index 32000\n",
      "Processing index 33000\n",
      "Processing index 34000\n",
      "Processing index 35000\n",
      "Processing index 36000\n",
      "(36395, 90)\n",
      "(36395, 90)\n",
      "(36395,)\n"
     ]
    }
   ],
   "source": [
    "with open('data/input_test.csv','rU') as csvfile:\n",
    "    dictfn = 'cache/ids_dictionary.pkl'\n",
    "    with open(dictfn) as dictfile:\n",
    "        ids_dict = pkl.load(dictfile)\n",
    "        \n",
    "    filereader = csv.reader(csvfile, delimiter=';')\n",
    "    \n",
    "    X_test = np.zeros((0, variables.MAX_LEN))\n",
    "    title_test = np.zeros((0, variables.MAX_LEN))\n",
    "    rating_test = []\n",
    "\n",
    "    for idx,row in enumerate(filereader):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        if (idx+1) % 1000 == 0 :\n",
    "            print 'Processing index {}'.format(idx+1)\n",
    "        \n",
    "        text = nlp(unicode(row[1], 'utf-8'))\n",
    "        X_test = np.concatenate((X_test, tokens2vec(text, ids_dict)))\n",
    "        \n",
    "        title = nlp(unicode(row[2], 'utf-8'))\n",
    "        title_test = np.concatenate((title_test, tokens2vec(title, ids_dict)))\n",
    "        \n",
    "        rating_test.append(int(row[3]))\n",
    "    \n",
    "    np.save('cache/test/data.npy', X_test)\n",
    "    np.save('cache/test/title.npy', title_test)\n",
    "    np.save('cache/test/rating.npy', np.asarray(rating_test)[:,np.newaxis])\n",
    "       \n",
    "    print X_test.shape\n",
    "    print title_test.shape\n",
    "    print np.asarray(rating_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
