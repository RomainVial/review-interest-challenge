{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import variables\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(vocab):\n",
    "    max_rank = max(lex.rank for lex in vocab if lex.has_vector)\n",
    "    vectors = np.zeros((max_rank+1, vocab.vectors_length), dtype='float32')\n",
    "    for lex in vocab:\n",
    "        if lex.has_vector:\n",
    "            vectors[lex.rank,:] = lex.vector\n",
    "    vectors = np.nan_to_num(vectors)\n",
    "    np.save('cache/embedding_weights.npy', vectors)\n",
    "    print 'Embeddings:',vectors.shape\n",
    "get_embeddings(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens2ids(tokens):\n",
    "    ids = np.zeros((variables.MAX_LEN,))\n",
    "    idx = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if idx >= variables.MAX_LEN:\n",
    "            break\n",
    "            \n",
    "        if token.is_space or token.is_punct or token.is_stop:\n",
    "            continue\n",
    "            \n",
    "        if token.has_vector:\n",
    "            ids[idx] = token.rank\n",
    "        else:\n",
    "            ids[idx] = 0\n",
    "        \n",
    "        idx+= 1\n",
    "            \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens2vec(tokens):\n",
    "    vec = np.zeros((nlp.vocab.vectors_length,))\n",
    "    count = 0.\n",
    "    \n",
    "    for token in tokens:           \n",
    "        if token.is_space or token.is_punct or token.is_stop:\n",
    "            continue\n",
    "            \n",
    "        if token.has_vector:\n",
    "            vec += token.vector\n",
    "            count += 1.\n",
    "    \n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "            \n",
    "    return np.nan_to_num(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(datafile, labelfile, savedir, phase, save=False):\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    \n",
    "    with open(datafile,'rU') as csvfile:        \n",
    "        filereader = csv.reader(csvfile, delimiter=';')\n",
    "        texts, titles, ratings = zip(*[(row[1], row[2], row[3]) for row in filereader])\n",
    "        \n",
    "    size = len(texts[1:])\n",
    "    idx_to_ignore = []\n",
    "\n",
    "    # Processing the review\n",
    "    X_text_vec = np.zeros((size, nlp.vocab.vectors_length))\n",
    "    X_text_ids = np.zeros((size, variables.MAX_LEN))\n",
    "    print '### Processing review content ###'\n",
    "    for idx,text in enumerate(texts[1:]):\n",
    "        if (idx+1) % 10000 == 0 :\n",
    "            print 'Index {}'.format(idx+1)\n",
    "        \n",
    "        doc = nlp.tokenizer(unicode(text, 'utf-8').lower())\n",
    "        X_text_vec[idx,:] = tokens2vec(doc)\n",
    "        X_text_ids[idx,:] = tokens2ids(doc)\n",
    "        \n",
    "        if np.amax(X_text_ids[idx,:]) == 0:\n",
    "            # Ignore if there are no embeddings for the review\n",
    "            idx_to_ignore.append(idx)\n",
    "\n",
    "    # Processing the title\n",
    "    X_titl_vec = np.zeros((size, nlp.vocab.vectors_length))\n",
    "    X_titl_ids = np.zeros((size, variables.MAX_LEN))\n",
    "    print '### Processing review title ###'\n",
    "    for idx,title in enumerate(titles[1:]):\n",
    "        if (idx+1) % 10000 == 0 :\n",
    "            print 'Index {}'.format(idx+1)\n",
    "            \n",
    "        doc = nlp.tokenizer(unicode(title, 'utf-8').lower())\n",
    "        X_titl_vec[idx,:] = tokens2vec(doc)\n",
    "        X_titl_ids[idx,:] = tokens2ids(doc)\n",
    "    \n",
    "    \n",
    "    # Processing the ratings\n",
    "    X_ratg = np.asarray(ratings[1:])[:,np.newaxis]\n",
    "    \n",
    "    # Processing the labels\n",
    "    if phase == 'train':\n",
    "        with open(labelfile,'rU') as csvfile:\n",
    "            filereader = csv.reader(csvfile, delimiter=';')\n",
    "            labels = [row[1] for row in filereader]\n",
    "        y = np.asarray(labels[1:])[:,np.newaxis]\n",
    "        \n",
    "    # Removing data whithout embedding\n",
    "    if phase == 'train':\n",
    "        X_text_vec = np.delete(X_text_vec, idx_to_ignore, axis=0).astype('float32')\n",
    "        X_text_ids = np.delete(X_text_ids, idx_to_ignore, axis=0).astype('int32')\n",
    "        X_titl_vec = np.delete(X_titl_vec, idx_to_ignore, axis=0).astype('float32')\n",
    "        X_titl_ids = np.delete(X_titl_ids, idx_to_ignore, axis=0).astype('int32')\n",
    "        X_ratg = np.delete(X_ratg, idx_to_ignore, axis=0).astype('float32')\n",
    "        y = np.delete(y, idx_to_ignore, axis=0).astype('int32')\n",
    "        \n",
    "    if save:\n",
    "        np.save(savedir+'text_vec.npy', X_text_vec)\n",
    "        np.save(savedir+'text_ids.npy', X_text_ids)\n",
    "        np.save(savedir+'titl_vec.npy', X_titl_vec)\n",
    "        np.save(savedir+'titl_ids.npy', X_titl_ids)\n",
    "        np.save(savedir+'ratg.npy', X_ratg)\n",
    "        if phase == 'train':\n",
    "            np.save(savedir+'labels.npy', y)\n",
    "\n",
    "    print 'Texts:', X_text_vec.shape, X_text_ids.shape\n",
    "    print 'Titles:', X_titl_vec.shape, X_titl_ids.shape\n",
    "    print 'Ratings:', X_ratg.shape\n",
    "    if phase == 'train':\n",
    "        print 'Labels:',y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datafile = 'data/input_train.csv'\n",
    "labelfile = 'data/output_train.csv'\n",
    "savedir = 'cache/train/'\n",
    "read_data(datafile, labelfile, savedir, 'train', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/input_test.csv'\n",
    "savedir = 'cache/test/'\n",
    "read_data(datafile, None, savedir, 'test', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
